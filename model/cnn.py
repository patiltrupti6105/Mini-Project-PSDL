# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z8J9p6rhDXhcvO6oFhID7mQ3VLReKkwc
"""

from google.colab import drive
drive.mount('/content/drive')

zip_path = '/content/drive/My Drive/for-2sec.zip'

import zipfile

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall('data')  # this extracts into /content/data/

import os

print(os.listdir('data'))

import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

# Path where your WAV files are stored
input_path = 'data/for-2seconds/training'  # organize as dataset/ai/*.wav and dataset/human/*.wav
output_path = '/content/drive/MyDrive/spectrograms/'  # where spectrogram images will be saved

os.makedirs(output_path + 'fake', exist_ok=True)
os.makedirs(output_path + 'real', exist_ok=True)

def save_spectrogram(wav_file, output_file):
    y, sr = librosa.load(wav_file, sr=None)  # Load the audio
    S = librosa.feature.melspectrogram(y=y, sr=sr)  # Mel-scaled spectrogram
    S_dB = librosa.power_to_db(S, ref=np.max)  # Convert to decibels

    plt.figure(figsize=(5, 5))
    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')
    plt.axis('off')  # No axes for clean input
    plt.tight_layout()
    plt.savefig(output_file, bbox_inches='tight', pad_inches=0)
    plt.close()

# Process all files
for label in ['fake', 'real']:
    folder = os.path.join(input_path, label)
    for file in os.listdir(folder):
        if file.endswith('.wav'):
            wav_file = os.path.join(folder, file)
            output_file = os.path.join(output_path, label, file.replace('.wav', '.png'))
            save_spectrogram(wav_file, output_file)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models

# Image dimensions
img_height, img_width = 224, 224

# Prepare dataset
train_datagen = ImageDataGenerator(
    rescale=1./255,  # rescale value to [0, 1] for stable training
    validation_split=0.2
)

train_generator = train_datagen.flow_from_directory(
    '/content/drive/MyDrive/spectrograms',
    target_size=(224, 224),
    batch_size=32, # num of images to be processed at once
    class_mode='binary', #binary classification
    subset='training',
    seed=123, # ensures reproducibility of dataset splittibg
    shuffle=True   # prevents overfitting
)

validation_generator = train_datagen.flow_from_directory(
    '/content/drive/MyDrive/spectrograms',
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    subset='validation',
    interpolation='nearest'   # nearest neighbor interpolation
)


# Build a simple CNN
model = models.Sequential([
    layers.Input(shape=(img_height, img_width, 3)), # 3 color channels
    layers.Conv2D(32, (3,3), activation='relu'), #first convolution layer
    layers.MaxPooling2D(2,2), #first pooling layer
    layers.Conv2D(64, (3,3), activation='relu'), #second convolution layer
    layers.MaxPooling2D(2,2), # second pooling layer
    layers.Flatten(), #converts 2D feature maps into 1D vector
    layers.Dense(64, activation='relu'), # fully connected layer - combines features

    layers.Dense(1, activation='sigmoid')  # Binary classification: AI or Human
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train
model.fit(
    train_generator,
    epochs=3,
    validation_data=validation_generator
)

from PIL import Image
import os

bad_files = []
base_dir = '/content/drive/MyDrive/spectrograms'

for folder in ['fake', 'real']:
    folder_path = os.path.join(base_dir, folder)
    for filename in os.listdir(folder_path):
        filepath = os.path.join(folder_path, filename)
        try:
            img = Image.open(filepath)
            img.verify()  # verify image
        except (IOError, SyntaxError) as e:
            print('Bad file:', filepath)
            bad_files.append(filepath)

print(f"Found {len(bad_files)} bad files.")

for f in bad_files:
    os.remove(f)

for label in ['fake', 'real']:
    folder = os.path.join(input_path, label)
    print(f"Looking into {folder}...")
    for file in os.listdir(folder):
        print(file)

import os
import random
import shutil

# Paths
base_path = 'data/for-2seconds/training/'
folders = ['fake', 'real']

for folder in folders:
    folder_path = os.path.join(base_path, folder)
    files = [f for f in os.listdir(folder_path) if f.endswith('.wav')]

    print(f"Found {len(files)} files in {folder}")

    # Shuffle files randomly
    random.shuffle(files)

    # Calculate half
    num_to_keep = len(files) // 2
    files_to_delete = files[num_to_keep:]

    # Delete the extra files
    for f in files_to_delete:
        file_path = os.path.join(folder_path, f)
        os.remove(file_path)

    print(f"After deletion, kept {num_to_keep} files in {folder}")

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import librosa
import librosa.display
import io

def predict_wav(wav_file):
    # 1. Load the WAV file
    y, sr = librosa.load(wav_file, sr=None)

    # 2. Create Mel Spectrogram
    S = librosa.feature.melspectrogram(y=y, sr=sr)
    S_dB = librosa.power_to_db(S, ref=np.max)

    # 3. Plot the spectrogram (exactly like your training images)
    fig = plt.figure(figsize=(5, 5))
    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')
    plt.axis('off')
    plt.tight_layout()

    # 4. Save figure to memory (not to disk)
    buf = io.BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)
    plt.close(fig)
    buf.seek(0)

    # 5. Read the spectrogram image
    img = tf.keras.preprocessing.image.load_img(buf, target_size=(224, 224))  # match your model input size
    img_array = tf.keras.preprocessing.image.img_to_array(img)
    img_array = img_array / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    # 6. Predict
    prediction = model.predict(img_array)

    # 7. Interpret result
    if prediction[0][0] <= 0.5:
        print("Prediction: FAKE voice ðŸŒŸ")
    else:
        print("Prediction: REAL human voice ðŸ§ ")

# Example usage
predict_wav('/content/drive/MyDrive/fakeyou_hgx26y8cwas099tykrxvmv99ctec1nfr.wav')
predict_wav('/content/drive/MyDrive/human.wav')

model.save('tfmodel.h5')
from google.colab import files
files.download('tfmodel.h5')